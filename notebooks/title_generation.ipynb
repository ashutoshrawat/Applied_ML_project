{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2)\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.summarization import summarize\n",
    "import re\n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the datasets that mainly have english titles\n",
    "df1 = pd.read_csv('archive/USvideos.csv')\n",
    "df2 = pd.read_csv('archive/CAvideos.csv')\n",
    "df3 = pd.read_csv('archive/GBvideos.csv')\n",
    "\n",
    "#load the datasets containing the category names\n",
    "data1 = json.load(open('archive/US_category_id.json'))\n",
    "data2 = json.load(open('archive/CA_category_id.json'))\n",
    "data3 = json.load(open('archive/GB_category_id.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_extractor(data):\n",
    "    i_d = [data['items'][i]['id'] for i in range(len(data['items']))]\n",
    "    title = [data['items'][i]['snippet'][\"title\"] for i in range(len(data['items']))]\n",
    "    i_d = list(map(int, i_d))\n",
    "    category = zip(i_d, title)\n",
    "    category = dict(category)\n",
    "    return category\n",
    "\n",
    "#create a new category column by mapping the category names to their id\n",
    "df1['category_title'] = df1['category_id'].map(category_extractor(data1))\n",
    "df2['category_title'] = df2['category_id'].map(category_extractor(data2))\n",
    "df3['category_title'] = df3['category_id'].map(category_extractor(data3))\n",
    "\n",
    "#join the dataframes\n",
    "df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "#drop rows based on duplicate videos\n",
    "df = df.drop_duplicates('video_id')\n",
    "\n",
    "#remove punctuations and convert text to lowercase\n",
    "def clean_text(text):\n",
    "    text = ''.join(e for e in text if e not in string.punctuation).lower()\n",
    "    \n",
    "    text = text.encode('utf8').decode('ascii', 'ignore')\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect only titles of entertainment videos\n",
    "entertainment = df[df['category_title'] == 'Entertainment']['title']\n",
    "entertainment = entertainment.tolist()\n",
    "\n",
    "corpus_entertainment = [clean_text(e) for e in entertainment]\n",
    "\n",
    "news = df[df['category_title'] == 'News & Politics']['title']\n",
    "news = news.tolist()\n",
    "\n",
    "corpus_news = [clean_text(e) for e in news]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entertainment            9730\n",
       "News & Politics          3415\n",
       "People & Blogs           3071\n",
       "Music                    2479\n",
       "Sports                   2422\n",
       "Comedy                   2305\n",
       "Howto & Style            1780\n",
       "Film & Animation         1431\n",
       "Gaming                    966\n",
       "Science & Technology      900\n",
       "Education                 763\n",
       "Pets & Animals            325\n",
       "Autos & Vehicles          310\n",
       "Travel & Events           247\n",
       "Shows                     106\n",
       "Nonprofits & Activism      14\n",
       "Movies                      1\n",
       "Name: category_title, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category_title'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "def get_sequence_of_tokens(corpus):\n",
    "  #get tokens\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    #convert to sequence of tokens\n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "    return input_sequences, total_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences,  maxlen=max_sequence_len, padding='pre'))\n",
    "    predictors, label = input_sequences[:,:-1], input_sequences[:, -1]\n",
    "    label = ku.np_utils.to_categorical(label, num_classes = total_words)\n",
    "    return predictors, label, max_sequence_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    " \n",
    "  # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    " \n",
    "  # Add Hidden Layer 1 â€” LSTM Layer\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1))\n",
    " \n",
    "  # Add Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    " \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2017539cb80>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model for entertainment\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus_entertainment)\n",
    "predictors, label, max_sequence_len_entertainment = generate_padded_sequences(inp_sequences)\n",
    "model_entertainment = create_model(max_sequence_len_entertainment, total_words)\n",
    "model_entertainment.fit(predictors, label, epochs=20, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x203561479a0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model for news and politics\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus_news)\n",
    "predictors, label, max_sequence_len_news = generate_padded_sequences(inp_sequences)\n",
    "model_news = create_model(max_sequence_len_news, total_words)\n",
    "model_news.fit(predictors, label, epochs=20, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1,  padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "\n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entertainment = df[df['category_title'] == 'Entertainment']\n",
    "df_news = df[df['category_title'] == 'News & Politics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words(\"english\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_frequecy(text):\n",
    "    URLless_string = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "    words = word_tokenize(URLless_string) \n",
    "    freqTable = {}\n",
    "    for word in words: \n",
    "        word = word.lower() \n",
    "        if word in stopWords: \n",
    "            continue\n",
    "        if '-' in word or '\\'' in word or  '&' in word or '?' in word or 'â€™' in word or '#' in word or '\\\\' in word or ';' in word or '(' in word or ')' in word or ':' in word or '!' in word or '.' in word or ',' in word or '|' in word:\n",
    "            continue\n",
    "        if word in freqTable: \n",
    "            freqTable[word] += 1\n",
    "        else: \n",
    "            freqTable[word] = 1\n",
    "    k = Counter(freqTable)\n",
    "    max_key =  k.most_common(3)\n",
    "    return max_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual title: I Dare You: GOING BALD!?\n",
      "\n",
      "Model Generate 3 Titles:\n",
      "\n",
      "Know At To A Whatcha And A 1St Nhn\n",
      "Since Rich Gi Episode Graham Hum\n",
      "Show In His Visit On Month The Jessica You\n",
      "\n",
      "When first word is provided:\n",
      "Bald Hoon A Goes To Tmz\n"
     ]
    }
   ],
   "source": [
    "row_num = 4\n",
    "des = df_entertainment.loc[row_num]['description']\n",
    "title = df_entertainment.loc[row_num]['title']\n",
    "print(f'Actual title: {title}')\n",
    "print()\n",
    "print('Model Generate 3 Titles:\\n')\n",
    "top_three = get_top_frequecy(des)\n",
    "for w in top_three:\n",
    "    print(generate_text(w[0], random.randint(3, 8), model_entertainment, max_sequence_len_entertainment))\n",
    "print()\n",
    "print('When first word is provided:')\n",
    "print(generate_text('Bald', random.randint(3, 8), model_entertainment, max_sequence_len_entertainment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual title: Rosie Oâ€™Donnell On Donald Trumpâ€™s Hostility Toward Her | WWHL\n",
      "\n",
      "Model Generate 3 Titles:\n",
      "\n",
      "Bravo Sunday Twitter Engtha20180430 To Rockets\n",
      "Wwhl Official History Minaj The The Last\n",
      "Donald Khi Dial Explained Haders Voice Teach\n",
      "\n",
      "When first word is provided:\n",
      "Trump 10 Down Roadies Kard\n"
     ]
    }
   ],
   "source": [
    "row_num = 75\n",
    "des = df_entertainment.loc[row_num]['description']\n",
    "title = df_entertainment.loc[row_num]['title']\n",
    "print(f'Actual title: {title}')\n",
    "print()\n",
    "print('Model Generate 3 Titles:\\n')\n",
    "top_three = get_top_frequecy(des)\n",
    "for w in top_three:\n",
    "    print(generate_text(w[0], random.randint(3, 8), model_entertainment, max_sequence_len_entertainment))\n",
    "print()\n",
    "print('When first word is provided:')\n",
    "print(generate_text('Trump', random.randint(3, 8), model_entertainment, max_sequence_len_entertainment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual title: Roy Moore & Jeff Sessions Cold Open - SNL\n",
      "\n",
      "Model Generate 3 Titles:\n",
      "\n",
      "Snl 10 Football Teaser 10 With\n",
      "Embattled Challenge To Les\n",
      "Alabama Juju Virtue The Gets Meghan About The\n",
      "\n",
      "When first word is provided:\n",
      "Jeff Official Newshour 34 What The Last\n"
     ]
    }
   ],
   "source": [
    "row_num = 6\n",
    "des = df_entertainment.loc[row_num]['description']\n",
    "title = df_entertainment.loc[row_num]['title']\n",
    "print(f'Actual title: {title}')\n",
    "print()\n",
    "print('Model Generate 3 Titles:\\n')\n",
    "top_three = get_top_frequecy(des)\n",
    "for w in top_three:\n",
    "    print(generate_text(w[0], random.randint(3, 8), model_entertainment, max_sequence_len_entertainment))\n",
    "print()\n",
    "print('When first word is provided:')\n",
    "print(generate_text('Jeff', random.randint(3, 8), model_entertainment, max_sequence_len_entertainment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9,13,28,41,51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual title: Why the rise of the robots wonâ€™t mean the end of work\n",
      "\n",
      "Model Generate 3 Titles:\n",
      "\n",
      "Work The Memo Is The\n",
      "Time Live House National For Friday\n",
      "Least Gonzalezs Live Trump National For Friday\n",
      "\n",
      "When first word is provided:\n",
      "Robots Is A Truth To The Queen Releasing\n"
     ]
    }
   ],
   "source": [
    "row_num = 9\n",
    "des = df_news.loc[row_num]['description']\n",
    "title = df_news.loc[row_num]['title']\n",
    "print(f'Actual title: {title}')\n",
    "print()\n",
    "print('Model Generate 3 Titles:\\n')\n",
    "top_three = get_top_frequecy(des)\n",
    "for w in top_three:\n",
    "    print(generate_text(w[0], random.randint(3, 8), model_news, max_sequence_len_news))\n",
    "print()\n",
    "print('When first word is provided:')\n",
    "print(generate_text('robots', random.randint(3, 8), model_news, max_sequence_len_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual title: Iraq-Iran earthquake: Deadly tremor hits border region - BBC News\n",
      "\n",
      "Model Generate 3 Titles:\n",
      "\n",
      "Least Gonzalezs Live Trump National\n",
      "Strong Franken On The White House Correspondents Dinner The\n",
      "Earthquake Ansari Dad Mom Spend The The\n",
      "\n",
      "When first word is provided:\n",
      "Region Live With Drshahid Masood 05December2017 Nawaz Sharif Asif\n"
     ]
    }
   ],
   "source": [
    "row_num = 41\n",
    "des = df_news.loc[row_num]['description']\n",
    "title = df_news.loc[row_num]['title']\n",
    "print(f'Actual title: {title}')\n",
    "print()\n",
    "print('Model Generate 3 Titles:\\n')\n",
    "top_three = get_top_frequecy(des)\n",
    "for w in top_three:\n",
    "    print(generate_text(w[0], random.randint(3, 8), model_news, max_sequence_len_news))\n",
    "print()\n",
    "print('When first word is provided:')\n",
    "print(generate_text('region', random.randint(3, 8), model_news, max_sequence_len_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual title: Lin-Manuel Miranda's next act: Helping rebuild Puerto Rico\n",
      "\n",
      "Model Generate 3 Titles:\n",
      "\n",
      "Cbs Conway Oliver Steps For The The World The\n",
      "Sunday Jones Day For Larry Nassar In\n",
      "Morning Joe January 14 2018 2018 President Trump Breaking\n",
      "\n",
      "When first word is provided:\n",
      "Puerto Farrow Is The The\n"
     ]
    }
   ],
   "source": [
    "row_num = 51\n",
    "des = df_news.loc[row_num]['description']\n",
    "title = df_news.loc[row_num]['title']\n",
    "print(f'Actual title: {title}')\n",
    "print()\n",
    "print('Model Generate 3 Titles:\\n')\n",
    "top_three = get_top_frequecy(des)\n",
    "for w in top_three:\n",
    "    print(generate_text(w[0], random.randint(3, 8), model_news, max_sequence_len_news))\n",
    "print()\n",
    "print('When first word is provided:')\n",
    "print(generate_text('Puerto', random.randint(3, 8), model_news, max_sequence_len_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "https://thecleverprogrammer.com/2020/10/05/title-generator-with-machine-learning/\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/text-summarization-approaches-nlp-example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
